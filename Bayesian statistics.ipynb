{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Bayesian Statisitcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Appendix](#Appendix)  \n",
    "[1. Highlights and rough work](#Highlights-and-rough-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlights and rough work\n",
    "a rough draft of my thoughts on first reading a chapter or section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 1\n",
    "<ul>\n",
    "<li>probability statements can only be made about random quantities, and in the frequentist approach the population parameters are held to be fixed. Thus, probability statements cannot be applied to them. Instead in the classical approach confidence statements, based on the probability distribution over all random samples, can be made regarding the parameters. The confidence is based on the average behavior of the process that generates the data.</li>\n",
    "</ul>\n",
    "\n",
    "The bayesian approach to statistics is represented by four main ideas:\n",
    "<ol>\n",
    "    <li>parameters are considered random variables</li>\n",
    "    <li>Rules of probability are used to make direct inference</li>\n",
    "    <li>Probability statements are a degree of belief, meaning the prior is subjective</li>\n",
    "    <li>We adapt our beliefs about the priors after seeing the data using bayes rule. This gives us the posterior</li>\n",
    "</ol>\n",
    "Our beliefs are updated based on the observed data, not all the data that could possibly have occured but didn't.\n",
    "\n",
    "<ul>\n",
    "<li>Bayesian stats has a general way of dealing with nuisance parameters, which frequentist do not</li>\n",
    "<li>Before taking the data into account we may want to know how well the model fits at certain values of the parameters. Our Priors are in effect distributions for our parameters and can therefore be used for such <em>pre-posterior</em> trials</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 2\n",
    "\n",
    "<ul>\n",
    "    <li><b>Randomized block designs:</b> When we know about a confounding variable we are able to control for it. Observatioins with similar levels of the lurking variable are <em>blocked together</em>. From these blocks they are spread equally between treatment groups. This ensures that each treatment has the same/similar mean value for that particular confounding variable while also controlling for other unidentified lurking variables.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 4\n",
    "<b>Important terms:</b>\n",
    "<ul>\n",
    "    <li>Joint probabiltiy</li>\n",
    "    <li>Marginal Probability</li>\n",
    "    <li>Conditional probabiltiy</li>\n",
    "    <li>Bayes theorem</li>\n",
    "    <li>Degree of belief assignment</li>\n",
    "    <li>Bayes factor</li>\n",
    "</ul>\n",
    "\n",
    "\\-Deductive logic can only deal with certainty  \n",
    "\\-IF propistion A is true then B is true. If A is in fact true then B is also True this is a deduction.  \n",
    "\\-Sample space: the set of all possible outcomes $\\Omega$ also known as the universe \n",
    "\\-Any set of possible outcomes for a random event.\n",
    "\\-Mutually exclusive= Disjoint sets\n",
    "  \n",
    "<b>Axioms of probability</b>\n",
    "\n",
    "1. $P(A)\\,\\le 0$\n",
    "2. $P(U)=1$\n",
    "3. If $P(A)$ and $P(B)$ are mutually exclusive $P(A\\cup B)$\n",
    "4. $P(A\\cup B) = P(A)+P(B)-P(A\\cap B)$\n",
    "\n",
    "The Joint probability of A and B is the probabilty that both events occur simultaneously on the same repition of the experiment. Which is the intersection. If A and B are <b>independent</b> then $P(A \\cap B)=P(A)\\times P(B)$. Independence is not a property of the events themselves, but rather of their intersection and their probabilities.\n",
    "\n",
    "<b>Marginal probability:</b> Is the probability of one event in the joint setting. Given by \n",
    "$A=(A \\cap B)\\cup (A\\cap \\bar{B})$\n",
    "\n",
    "<b>Conditional Probability:</b> If event A has occurred then nothing outside event A is possible. The reduced univers is now $U_{r}=A$.\n",
    "  \n",
    "<b>Conditional Probability for independent events:</b> Knowledge about A does not affect the probabiliity of B occurring. \n",
    "\n",
    "<b>Multiplication rule:</b> $P(A \\cap B)=P(B)\\times P(A|B) $\n",
    "  \n",
    "<b>*Bayes theorem</b>  \n",
    "From conditional probabilty we know marginal probability is found by summing the <em>mutually exclusive/disjoint</em> parts of an Event say A. Thus we substitute $A=(A \\cap B)\\cup (A\\cap \\bar{B})$ into $P(B|A)=\\frac{P(B \\cap A)}{P(A)}$ and we get $$P(B|A)=\\frac{P(B \\cap A)}{(A \\cap B)\\cup (A\\cap \\bar{B})}$$ Then we use the multiplication rule to find the joint probabilities, and that gives us bayes rule: $$P(B|A)=\\frac{P(B \\cap A)}{(A|B)\\times P(B)+ (A|\\bar{B})\\times P(\\bar{B})}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 ('Kernel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9738b5824fcb5c601bc7da97e16a30cd4d8bc69ceef6489b1986c3e5b099a82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
