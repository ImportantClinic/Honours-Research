{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Bayesian Statisitcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Appendix](#Appendix)  \n",
    "[1. Highlights and rough work](#Highlights-and-rough-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlights and rough work\n",
    "a rough draft of my thoughts on first reading a chapter or section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 1\n",
    "<ul>\n",
    "<li>probability statements can only be made about random quantities, and in the frequentist approach the population parameters are held to be fixed. Thus, probability statements cannot be applied to them. Instead in the classical approach confidence statements, based on the probability distribution over all random samples, can be made regarding the parameters. The confidence is based on the average behavior of the process that generates the data.</li>\n",
    "</ul>\n",
    "\n",
    "The bayesian approach to statistics is represented by four main ideas:\n",
    "<ol>\n",
    "    <li>parameters are considered random variables</li>\n",
    "    <li>Rules of probability are used to make direct inference</li>\n",
    "    <li>Probability statements are a degree of belief, meaning the prior is subjective</li>\n",
    "    <li>We adapt our beliefs about the priors after seeing the data using bayes rule. This gives us the posterior</li>\n",
    "</ol>\n",
    "Our beliefs are updated based on the observed data, not all the data that could possibly have occured but didn't.\n",
    "\n",
    "<ul>\n",
    "<li>Bayesian stats has a general way of dealing with nuisance parameters, which frequentist do not</li>\n",
    "<li>Before taking the data into account we may want to know how well the model fits at certain values of the parameters. Our Priors are in effect distributions for our parameters and can therefore be used for such <em>pre-posterior</em> trials</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 2\n",
    "\n",
    "<ul>\n",
    "    <li><b>Randomized block designs:</b> When we know about a confounding variable we are able to control for it. Observatioins with similar levels of the lurking variable are <em>blocked together</em>. From these blocks they are spread equally between treatment groups. This ensures that each treatment has the same/similar mean value for that particular confounding variable while also controlling for other unidentified lurking variables.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 4\n",
    "<b>Important terms:</b>\n",
    "<ul>\n",
    "    <li>Joint probabiltiy</li>\n",
    "    <li>Marginal Probability</li>\n",
    "    <li>Conditional probabiltiy</li>\n",
    "    <li>Bayes theorem</li>\n",
    "    <li>Degree of belief assignment</li>\n",
    "    <li>Bayes factor</li>\n",
    "</ul>\n",
    "\n",
    "\\-Deductive logic can only deal with certainty  \n",
    "\\-IF propistion A is true then B is true. If A is in fact true then B is also True this is a deduction.  \n",
    "\\-Sample space: the set of all possible outcomes $\\Omega$ also known as the universe \n",
    "\\-Any set of possible outcomes for a random event.\n",
    "\\-Mutually exclusive= Disjoint sets\n",
    "  \n",
    "<b>Axioms of probability</b>\n",
    "\n",
    "1. $P(A)\\,\\le 0$\n",
    "2. $P(U)=1$\n",
    "3. If $P(A)$ and $P(B)$ are mutually exclusive $P(A\\cup B)$\n",
    "4. $P(A\\cup B) = P(A)+P(B)-P(A\\cap B)$\n",
    "\n",
    "The Joint probability of A and B is the probabilty that both events occur simultaneously on the same repition of the experiment. Which is the intersection. If A and B are <b>independent</b> then $P(A \\cap B)=P(A)\\times P(B)$. Independence is not a property of the events themselves, but rather of their intersection and their probabilities.\n",
    "\n",
    "<b>Marginal probability:</b> Is the probability of one event in the joint setting. Given by \n",
    "$A=(A \\cap B)\\cup (A\\cap \\bar{B})$\n",
    "\n",
    "<b>Conditional Probability:</b> If event A has occurred then nothing outside event A is possible. The reduced universe is now $U_{r}=A$.\n",
    "  \n",
    "<b>Conditional Probability for independent events:</b> Knowledge about A does not affect the probabiliity of B occurring. \n",
    "\n",
    "<b>Multiplication rule:</b> $P(A \\cap B)=P(B)\\times P(A|B) $\n",
    "  \n",
    "<b>*Bayes theorem</b>  \n",
    "From conditional probabilty we know marginal probability is found by summing the <em>mutually exclusive/disjoint</em> parts of an Event say A. Thus we substitute $A=(A \\cap B)\\cup (A\\cap \\bar{B})$ into $P(B|A)=\\frac{P(B \\cap A)}{P(A)}$ and we get $$P(B|A)=\\frac{P(B \\cap A)}{(A \\cap B)\\cup (A\\cap \\bar{B})}$$ Then we use the multiplication rule to find the joint probabilities, and that gives us bayes rule: $$P(B|A)=\\frac{P(B \\cap A)}{P(A|B)\\times P(B)+ P(A|\\bar{B})\\times P(\\bar{B})}$$\n",
    "\n",
    "-The likelihood of unobservable events $B_1,...,B_n$ is the conditional probability that A has occured, given $B_i$\n",
    "\n",
    "-$P(B_i|A)$ is the posterior probability of the event $B_i$ given that event A has occurred. This distribution contains the weight $B_i$ we assign to each event after we know that A has happened.\n",
    "\n",
    "<b>The bayesian universe</b>\n",
    "<ul>\n",
    "    <li>there are 2 dimensions: observable and <em>unobservable</em></li>\n",
    "    <li>hoizontal-observable</li>\n",
    "    <li>vertical-unobservable</li>\n",
    "    <li>In bayes theorem each of the joint probabilities is found by multiplying the prior by the likelihood</li>\n",
    "</ul>\n",
    "\n",
    "Multiplying the numerator by a constant results in the same happening to the denominator, thus the constants cancel out. This means we only have to know the likelihood to within a constant of proportionality. This gives us the proportional form: $$posterior \\propto likelihood \\times prior$$\n",
    "\n",
    "The odds of an event are the probability of it happening divided by its complement.\n",
    "$$odds(C) = \\frac{P(C)}{P(\\bar{C})}$$\n",
    "\n",
    "<b>Bayes Factor</b>\n",
    "- the factor by which the prior odds are adjusted to become the posterior odds $B= \\frac{posterior\\; odds}{prior\\; odds}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 5\n",
    "Important terms:\n",
    "<ul>\n",
    "    <li>Hyper geometric distribution</li>\n",
    "    <li>Poisson</li>\n",
    "    <li>Characteristics of the Poisson</li>\n",
    "    <li>Joint random</li>\n",
    "    <li>Independent random</li>\n",
    "    <li>Conditional Probability for Joint random variables</li>\n",
    "\n",
    "</ul>\n",
    "  \n",
    "<b>Hyper Geometric distribution</b>  \n",
    "Represents sampling from a population without replacement. I.e. The probability of a success $\\pi$ changes. The pdf is given by:$$f(y|N,R,n)=\\frac{\\binom{R}{y}\\times \\binom{N-R}{n-y}}{\\binom{N}{n}}$$\n",
    "\n",
    "<b>Poisson distribution</b>  \n",
    "counts the number of rare occurences over a period. The number of trials in a poisson is unknown. \n",
    "\n",
    "<b>Independent Random probability</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 ('Kernel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9738b5824fcb5c601bc7da97e16a30cd4d8bc69ceef6489b1986c3e5b099a82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
